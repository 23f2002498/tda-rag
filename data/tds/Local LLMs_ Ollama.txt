<article class="markdown-section" id="main"><h2 id="local-llm-runner-ollama"><a class="anchor" data-id="local-llm-runner-ollama" href="#/ollama?id=local-llm-runner-ollama"><span>Local LLM Runner: Ollama</span></a></h2><p><a href="https://github.com/ollama/ollama" rel="noopener" target="_blank"><code>ollama</code></a> is a command-line tool for running open-source large language models entirely on your own machine—no API keys, no vendor lock-in, full control over models and performance.</p><p><a href="https://youtu.be/Lb5D892-2HY" rel="noopener" target="_blank"><img alt="Run AI Models Locally: Ollama Tutorial (Step-by-Step Guide + WebUI)" data-origin="https://i.ytimg.com/vi_webp/Lb5D892-2HY/sddefault.webp" src="https://i.ytimg.com/vi_webp/Lb5D892-2HY/sddefault.webp"/></a></p><h3 id="basic-usage"><a class="anchor" data-id="basic-usage" href="#/ollama?id=basic-usage"><span>Basic Usage</span></a></h3><p><a href="https://ollama.com/" rel="noopener" target="_blank">Download Ollama for macOS, Linux, or Windows</a> and add the binary to your <code>PATH</code>. See the full <a href="https://ollama.com/docs" rel="noopener" target="_blank">Docs ↗</a> for installation details and troubleshooting.</p><pre class="language-bash" data-lang="bash" v-pre=""><code class="lang-bash language-bash"><span class="token comment"># List installed and available models</span>
ollama list

<span class="token comment"># Download/pin a specific model version</span>
ollama pull gemma3:1b-it-qat

<span class="token comment"># Run a one-off prompt</span>
ollama run gemma3:1b-it-qat <span class="token string">'Write a haiku about data visualization'</span>

<span class="token comment"># Launch a persistent HTTP API on port 11434</span>
ollama serve

<span class="token comment"># Interact programmatically over HTTP</span>
<span class="token function">curl</span> <span class="token parameter variable">-X</span> POST http://localhost:11434/api/chat <span class="token punctuation">\</span>
     <span class="token parameter variable">-H</span> <span class="token string">'Content-Type: application/json'</span> <span class="token punctuation">\</span>
     <span class="token parameter variable">-d</span> <span class="token string">'{"model":"gemma3:1b-it-qat","prompt":"Hello, world!"}'</span></code><button class="docsify-copy-code-button"><span class="label">Copy to clipboard</span><span aria-hidden="hidden" class="error">Error</span><span aria-hidden="hidden" class="success">Copied</span><span aria-live="polite"></span></button></pre><h3 id="key-features"><a class="anchor" data-id="key-features" href="#/ollama?id=key-features"><span>Key Features</span></a></h3><ul><li><strong>Model management</strong>: <code>list</code>/<code>pull</code> — Install and switch among Llama 3.3, DeepSeek-R1, Gemma 3, Mistral, Phi-4, and more.</li><li><strong>Local inference</strong>: <code>run</code> — Execute prompts entirely on-device for privacy and zero latency beyond hardware limits.</li><li><strong>Persistent server</strong>: <code>serve</code> — Expose a local REST API for multi-session chats and integration into scripts or apps.</li><li><strong>Version pinning</strong>: <code>pull model:tag</code> — Pin exact model versions for reproducible demos and experiments.</li><li><strong>Resource control</strong>: <code>--threads</code> / <code>--context</code> — Tune CPU/GPU usage and maximum context window for performance and memory management.</li></ul><h3 id="real-world-use-cases"><a class="anchor" data-id="real-world-use-cases" href="#/ollama?id=real-world-use-cases"><span>Real-World Use Cases</span></a></h3><ul><li><strong>Quick prototyping</strong>. Brainstorm slide decks or blog outlines offline, without worrying about API quotas: <code>ollama run gemma-3 'Outline a slide deck on Agile best practices'</code></li><li><strong>Data privacy</strong>. Summarize sensitive documents on-device, retaining full control of your data: <code>cat financial_report.pdf | ollama run phi-4 'Summarize the key findings'</code></li><li><strong>CI/CD integration</strong>. Validate PR descriptions or test YAML configurations in your pipeline without incurring API costs: <code>git diff origin/main | ollama run llama2 'Check for style and clarity issues'</code></li><li><strong>Local app embedding</strong>. Power a desktop or web app via the local REST API for instant LLM features: <code>curl -X POST http://localhost:11434/api/chat -d '{"model":"mistral","prompt":"Translate to German"}'</code></li></ul><p>Read the full <a href="https://github.com/ollama/ollama/tree/main/docs" rel="noopener" target="_blank">Ollama docs ↗</a> for advanced topics like custom model hosting, GPU tuning, and integrating with your development workflows.</p><div class="docsify-pagination-container">
<div class="pagination-item pagination-item--previous">
<a href="#/google-auth">
<div class="pagination-item-label">
<svg height="16" viewbox="0 0 10 16" width="10" xmlns="http://www.w3.org/2000/svg">
<polyline fill="none" points="8,2 2,8 8,14" vector-effect="non-scaling-stroke"></polyline>
</svg>
<span>Previous</span>
</div>
<div class="pagination-item-title">Authentication: Google Auth</div>
<div class="pagination-item-subtitle"></div></a>
</div>
<div class="pagination-item pagination-item--next">
<a href="#/large-language-models">
<div class="pagination-item-label">
<span>Next</span>
<svg height="16" viewbox="0 0 10 16" width="10" xmlns="http://www.w3.org/2000/svg">
<polyline fill="none" points="2,2 8,8 2,14" vector-effect="non-scaling-stroke"></polyline>
</svg>
</div>
<div class="pagination-item-title">3. Large Language Models</div>
<div class="pagination-item-subtitle"></div></a>
</div>
</div></article>